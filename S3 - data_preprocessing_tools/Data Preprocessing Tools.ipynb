{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3.7.0 64-bit ('base': conda)","language":"python","name":"python37064bitbaseconda884dfe83e9dd49ba8c885bd5253ac65d"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"Data Preprocessing Tools.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"qimxupj1tfMO"},"source":["# Data Preprocessing Tools\n","\n","## Importing the libraries"]},{"cell_type":"code","metadata":{"id":"yelbAOTdtfMP"},"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dqDxUgDhtfMV"},"source":["## Importing the dataset\n","For now I know only this method how to read data - setting full path in Jupyter.\n","\n","Google Colab can colaborate with GitHub, but problem is loading data files to the notebooks. \n","\n","**SOLVE!** You have to open your notebook from your google drive and then upload your file in column on the left of your display :).\n","\n","- features (also independant variables), data that contained in columns\n","- dependent variable - value we want to predict"]},{"cell_type":"code","metadata":{"id":"_xVN6JNAtfMW"},"source":["dataset = pd.read_csv('c:/Users/to068616/Disk Google/Colab Notebooks/UDEMY - 1 - data_preprocessing_tools/Data.csv')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cHXursRQtfMa"},"source":["x = dataset.iloc[:, :-1].values #take all raws and all columns but the last\n","y = dataset.iloc[:, -1].values #take all raws and take the last column"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AUrimWIktfMe","outputId":"55cd3737-8974-464b-eb4b-e0b85d3190df"},"source":["print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 nan]\n"," ['France' 35.0 58000.0]\n"," ['Spain' nan 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zt-dxq3stfMi","outputId":"d31ddda1-ffec-4468-ce4e-dec9ddf32295"},"source":["print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['No' 'Yes' 'No' 'No' 'Yes' 'Yes' 'No' 'Yes' 'No' 'Yes']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"aKJXw9tZtfMp"},"source":["## Taking care of missing data\n","Few methods\n","- if less then 1% of data missing we can ignore them\n","- we can replace them (for example with average, medium, most frequent)\n","\n","OK, so here come first tool I've never know so far - **SimpleImputer** from **scikit-learn** library. SimpleImputer replace all missing_values (more specifically by *missing_values* argument) by some value specified by *strategy* argument."]},{"cell_type":"code","metadata":{"id":"9W4Whvv7tfMq"},"source":["from sklearn.impute import SimpleImputer\n","imputer = SimpleImputer(missing_values = np.nan, strategy='mean')\n","imputer.fit(x[:, 1:3])\n","x[:, 1:3] = imputer.transform(x[:, 1:3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x2tJYwsftfMv","outputId":"bf2099b4-f5d6-428b-8f71-a20bb187b824"},"source":["print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[['France' 44.0 72000.0]\n"," ['Spain' 27.0 48000.0]\n"," ['Germany' 30.0 54000.0]\n"," ['Spain' 38.0 61000.0]\n"," ['Germany' 40.0 63777.77777777778]\n"," ['France' 35.0 58000.0]\n"," ['Spain' 38.77777777777778 52000.0]\n"," ['France' 48.0 79000.0]\n"," ['Germany' 50.0 83000.0]\n"," ['France' 37.0 67000.0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"b2bLPWpetfM4"},"source":["## Encoding categorical data\n","We want to avoid to replace categorical data (like 'Male' and 'Female') by number because learning models can create some numerical order between them and this could misinterpret some correlation.\n","\n","*Example: Male would be 0 and Female would be 1. Male is less than Female.*\n","\n","### Handling independent variable\n","Firstly, let's handle Country column. It is a column with categorical data. We create so called **dummy variables**.\n","\n","**Dummy variable trap** - if there is a strong correlation between two dummy variable, we should omit one of them in a model (because it is redundant). But model we will use usually avoid this trap.\n","\n","*Example - two dummy variables Male/Female. It is very clear that the one who is not Male is Female.*"]},{"cell_type":"code","metadata":{"id":"p8qlZfABtfM5","outputId":"f76a03f5-75a8-466b-bf30-a63c6152cb94"},"source":["from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder\n","\n","ct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\n","x = np.array(ct.fit_transform(x)) # we need to transform to np array\n","\n","print(x)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [0.0 1.0 0.0 30.0 54000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 35.0 58000.0]\n"," [0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"IZqJXYqetfM-"},"source":["### Handling dependent variable\n","and secondo let's handle the dependant variable (y)"]},{"cell_type":"code","metadata":{"id":"ArcNGu53tfM-","outputId":"754b0a5e-8568-4e10-b720-4b38bdb7f57d"},"source":["from sklearn.preprocessing import LabelEncoder # we can use label encoding when we have binary data (two categories)\n","le = LabelEncoder()\n","y = le.fit_transform(y)\n","\n","print(y)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 1 0 0 1 1 0 1 0 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oVlp0sqntfNC"},"source":["## Splitting the dataset into the Training set and Test set\n","Recommanded sizes\n","80% train size\n","20% test size\n"]},{"cell_type":"code","metadata":{"id":"CyPgsN4-tfNC"},"source":["from sklearn.model_selection import train_test_split\n","\n","x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r64IL2q8tfNG","outputId":"e36ca95f-5ef3-4513-d0eb-b0bcbdab9846"},"source":["print(x_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.0 0.0 1.0 38.77777777777778 52000.0]\n"," [0.0 1.0 0.0 40.0 63777.77777777778]\n"," [1.0 0.0 0.0 44.0 72000.0]\n"," [0.0 0.0 1.0 38.0 61000.0]\n"," [0.0 0.0 1.0 27.0 48000.0]\n"," [1.0 0.0 0.0 48.0 79000.0]\n"," [0.0 1.0 0.0 50.0 83000.0]\n"," [1.0 0.0 0.0 35.0 58000.0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"eb3TGG_ctfNL","outputId":"ad5da439-fe26-4346-cb1b-080377a40ac4"},"source":["print(x_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.0 1.0 0.0 30.0 54000.0]\n"," [1.0 0.0 0.0 37.0 67000.0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"T1pcSmTrtfNP","outputId":"00469163-b3d2-4dbb-c513-7d5e91ac28b4"},"source":["print(y_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 1 0 0 1 1 0 1]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"6wGwHPJ5tfNT","outputId":"29fb7df6-ef5b-49ca-8a2d-9c0cc3b60e15"},"source":["print(y_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0 1]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"I39rQ2FatfNY"},"source":["## Feature Scaling\n","This has to be after splitting the dataset, because we don't want to influence Test set by data from Training set.\n","And we do this because some models (not all, for example regressions) don't want to some feature dominate.\n","\n","*Example: age 27 is much less than salary (or whatever) with value 83000.*\n","\n","Let's use two techniques:\n","**Standardisation** (always works) and **Normalisation** (works with normal distribution, which features usually are - but not always).\n","Let's use **Standardisation** respectively.\n","We don't use it to **dummy variables**."]},{"cell_type":"code","metadata":{"id":"SsUT3xiEtfNZ"},"source":["from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","x_train[:, 3:] = sc.fit_transform(x_train[:, 3:])\n","x_test[:, 3:] = sc.transform(x_test[:, 3:])#we use the seam scaler, we don't need to train new scaler"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8bNHa9ftfNc","outputId":"362f4e8f-e173-43dc-aedc-d702aa389184"},"source":["print(x_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.0 0.0 1.0 -0.19159184384578545 -1.0781259408412425]\n"," [0.0 1.0 0.0 -0.014117293757057777 -0.07013167641635372]\n"," [1.0 0.0 0.0 0.566708506533324 0.633562432710455]\n"," [0.0 0.0 1.0 -0.30453019390224867 -0.30786617274297867]\n"," [0.0 0.0 1.0 -1.9018011447007988 -1.420463615551582]\n"," [1.0 0.0 0.0 1.1475343068237058 1.232653363453549]\n"," [0.0 1.0 0.0 1.4379472069688968 1.5749910381638885]\n"," [1.0 0.0 0.0 -0.7401495441200351 -0.5646194287757332]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"kv4UCPWDtfNl","outputId":"5c04485a-7f3e-4add-f423-5e406f0bb7aa"},"source":["print(x_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[0.0 1.0 0.0 -1.4661817944830124 -0.9069571034860727]\n"," [1.0 0.0 0.0 -0.44973664397484414 0.2056403393225306]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B_fAGkkgtfNq"},"source":[""],"execution_count":null,"outputs":[]}]}